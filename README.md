# HessianFreeImplementation
This repository is for exploration into different optimization methods for deep learning. The initial intention was to use Hessian Free Optimization (using finite difference and approximating Hd, where d is the search direction, with Gd using Conjugate Gradient, and many other suggestions) presented in J. Martens (2010) with the proposed positive definite Hessian to avoid Saddle Points in high dimensional space (Dauphin et al, 2014)

Used for IE 598 Optimization of Large Scale Systems at Univeristy of Illinois at Urbana-Champaign.
